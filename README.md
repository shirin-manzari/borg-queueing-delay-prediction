# Project overview
- **Goal:** Predict per-task queueing delay in the Google 2011 cluster trace using the full raw CSVs (task_events, machine_events, etc.). Target = first SCHEDULE minus first SUBMIT in seconds.
- **Approach:** Keep all heavy data processing inside DuckDB to avoid RAM blow-ups, build a feature table in Parquet, then train an XGBoost regressor (hist method) with log1p + clipping for heavy-tailed delays. Provide validation, baselines, pre-training EDA, and post-training evaluation with plots.
- **Entrypoint:** `scripts/run_pipeline.py` orchestrates validation → feature build → pre-training analysis → training → evaluation. Every stage can be skipped with flags.

# Files and functions
## Top-level
- `explanation.md`: Project roadmap and problem framing for queueing-delay prediction on the Google 2011 trace.
- `Google cluster-usage traces format schema 2014-11-17 external.pdf`: Official schema/spec for the raw CSVs.
- `PIPELINE.md`: Pipeline usage, stages, outputs, and tips.
- `doc.txt` / `doc.md`: In-repo documentation explaining the system, files, and functions.
- `extracted/`: Raw CSV inputs (task_events.csv, machine_events.csv, job_events.csv, etc.).
- `artifacts/`: Outputs generated by the pipeline (reports, Parquet features, models, metrics, plots).
- `scripts/`: Python package containing all pipeline stages.

## scripts/common.py
- Constants: `BASE_DIR`, `DATA_DIR` (raw CSVs), `ARTIFACTS_DIR` (outputs), `DB_PATH`.
- Schemas: `*_COLUMNS` dicts for each CSV/table (types from the Google schema PDF), using `UBIGINT` for IDs/timestamps to handle 64-bit unsigned values.
- `_duckdb_config()`: builds a safe DuckDB config (temp dir under artifacts, memory limit, threads) with env overrides.
- `connect_duckdb(read_only=False)`: creates the DuckDB file (if missing) with the spill-to-disk config.
- `_read_csv_sql(path, columns)`: helper to build a `read_csv` SQL with explicit column types and no auto-detect.
- `register_views(con)`: creates/replaces DuckDB views over all raw CSVs using explicit schemas to avoid inference on huge files.
- `ensure_database()`: convenience wrapper to connect and register views.
- `MODEL_FEATURE_COLUMNS`: ordered list of feature columns used for training/eval.
- Notes: temp dir is `artifacts/duckdb_tmp`; override memory/threads via `DUCKDB_MEMORY_LIMIT`, `DUCKDB_THREADS`.

## scripts/validate_raw.py
- `_df_section(title, df)`: formats a pandas DataFrame into a titled text block.
- `run_validation()`: connects to DuckDB and runs:
  - Row counts per raw table.
  - Task stats (distinct tasks/jobs, min/max timestamps, event_type bounds/invalids, priority/resource bounds, missing_info count).
  - Task event distribution by event_type.
  - Priority histogram (top 50 values).
  - Machine event distribution.
  - Machine capacity min/max (CPU/memory).
  - Task usage min/max for key fields and invalid aggregation types.
  - Writes `artifacts/validation_report_duckdb.txt`.
- `main()`: executes `run_validation()` and reports the path.
- Notes: all queries stay inside DuckDB; avoids loading entire CSVs into RAM.

## scripts/build_features.py
- `build_feature_table()`: end-to-end feature creation inside DuckDB:
  - `minute_events`: per-minute counts of submits/schedules from task_events.
  - `backlog`: cumulative pending estimate from minute_events windowed sum.
  - `submits`: first SUBMIT timestamp per task.
  - `schedules`: first SCHEDULE timestamp/machine per task.
  - `task_attributes`: per-task arg_min of priority/class/requests + flags (different_machine_constraint, missing_info).
  - `job_task_counts`: tasks per job.
  - `machine_capacity`: latest CPU/memory per machine (arg_max by timestamp).
  - `features`: joins the above, computes queue_delay_seconds, temporal buckets (day/hour/minute), load features (backlog, submits/schedules_same_minute), request_sum, machine capacity, and deterministic split via `abs(hash(job_id)) % 100` (80/10/10).
  - Writes `artifacts/features.parquet` (ZSTD) and `artifacts/feature_summary.txt`.
- `main()`: runs build and prints outputs.
- Notes: stays in DuckDB to handle the ~186GB raw CSVs; large row groups improve downstream streaming.

## scripts/baseline_metrics.py
- `compute_baselines(features_path=None)`: reads features parquet via DuckDB; computes train distribution summary (mean/median/p90/p95/p99/p99.9/max) and evaluates constant mean/median predictors for each split (MAE/RMSE). Writes `artifacts/baseline_metrics.txt`.
- `main()`: runs and reports the path.
- Notes: provides quick reference points to gauge model improvement.

## scripts/pretrain_analysis.py
- Purpose: quick EDA before training.
- `analyze(features_path, clip_target, sample_cap)`: samples up to `sample_cap` rows; clips labels if set; computes label quantiles overall/by split; plots queue delay histogram, first 8 feature histograms, and correlation heatmap (if seaborn available). Writes `artifacts/pretrain_metrics.json` and plots (`pretrain_queue_delay_dist.png`, `pretrain_feature_*.png`, `pretrain_feature_corr.png`).
- `main()`: runs with defaults (clip 7200s, sample 500k).

## scripts/train_model.py
- Uses XGBoost with streaming `QuantileDMatrix` to avoid loading full Parquet.
- `_transform_labels(series, clip_target, log1p_target)`: clips and optionally log1p-transforms labels.
- `ParquetIterator`: `xgb.DataIter` over a split; loads batches, applies label transform, drops split column, selects model feature columns, and feeds XGBoost.
- `train(features_path, ...)`: builds train/valid iterators; sets hist-tree params (lr 0.05, depth 10, min_child_weight 2, subsample/colsample 0.9, 800 rounds with early stopping); trains; saves `artifacts/xgb_queue_delay.json` and `artifacts/model_metrics.json`; returns booster + metrics.
- `evaluate_split(...)`: streams a split, predicts, inverts log1p (with clipping), clamps predictions to clip target, computes RMSE/MAE in seconds.
- CLI: boosting params, batch size, early stopping, clip target, `--no-log1p-target` to disable log transform.
- `main()`: parses args, runs train, prints tree count and metrics.
- Notes: defaults assume heavy-tailed targets; streaming `QuantileDMatrix` keeps memory bounded.

## scripts/evaluate_model.py
- Post-training evaluation with sampling and plots; supports log-trained models.
- `_stream_metrics(booster, dataset, filter_expr, batch_size, clip_target, sample_cap, log1p_target)`: streams batches, inverts log1p if needed, clips preds to [0, clip_target], computes RMSE/MAE and abs-error quantiles; reservoir-samples points for plots.
- `_baseline_stats(dataset, clip_target)`: train mean/median with optional clipping.
- `_baseline_metrics(...)`: evaluates mean/median baselines on a subset.
- `_decile_summary(samples)`: quantiles for actual/pred/abs-error over sampled points.
- `_feature_importance(booster)`: top-k feature importance by gain/weight/cover.
- `_make_plots(samples, output_path)`: diagnostics (pred vs actual, residuals, abs-error hist, abs-error CDF, abs-error quantile bars, residuals vs actual) and saves an additional predicted-vs-actual distribution plot with `_dist` suffix.
- `evaluate(...)`: orchestrates evaluation on test split and a random sample; computes baselines, deciles, feature importance; writes `artifacts/evaluation_metrics.json` and plots (`evaluation_plots.png`, `evaluation_random_plots.png`, plus `_dist` variants).
- CLI flag `--no-log1p-target` if the model was trained without log transform.
- Notes: clamps predictions to non-negative; random sample offers all-split sanity check.

## scripts/run_pipeline.py
- Orchestration CLI with skip flags: `--skip-validate`, `--skip-features`, `--skip-pretrain-analysis`, `--skip-train`, `--skip-eval`.
- Steps:
  1) Validation (`validate_raw.run_validation`).
  2) Feature build (`build_features.build_feature_table`) or reuse `--features-path`.
  3) Pre-training EDA (`pretrain_analysis.analyze`).
  4) Training (`train_model.train`) with provided hyperparameters, clipping, log1p flag.
  5) Evaluation (`evaluate_model.evaluate`) unless skipped.
- Hyperparameters mirror `train_model.py`; default clip 7200s and log1p enabled. Prints paths and plot locations.

## scripts/validate_raw.py
- See above; used by the pipeline in step 1.

## scripts/__init__.py
- Marks `scripts` as a package.

# How the system fits together
1) **Validation**: `run_pipeline` → `validate_raw.run_validation()` checks raw CSV schema/ranges.
2) **Feature build**: `build_features.build_feature_table()` constructs `features.parquet` with queue delay targets and engineered features, plus split assignment.
3) **Pre-train EDA**: `pretrain_analysis.analyze()` samples `features.parquet` to visualize label/feature distributions and correlations.
4) **Baselines (optional)**: `baseline_metrics.compute_baselines()` gives constant predictors for comparison.
5) **Training**: `train_model.train()` streams train/valid splits, trains hist-based XGBoost on log1p(clipped) targets, writes model + metrics.
6) **Evaluation**: `evaluate_model.evaluate()` streams test and random subsets, computes metrics vs baselines, error quantiles, feature importance, and generates diagnostic plots.
7) **Artifacts**: All outputs land in `artifacts/` (reports, metrics JSON/txt, Parquet features, model, plots).

# Core concepts and rationale
- **Queueing delay target**: Derived from first SUBMIT and SCHEDULE events per task (microseconds → seconds). Heavy-tailed; handled via clipping and optional log1p transform.
- **Streaming + DuckDB**: Raw CSVs are huge (~186GB); all aggregation is done in DuckDB views/tables, and model training/eval streams Parquet batches to avoid memory blow-ups.
- **Feature design**: Combines static task/job attributes (priority, class, requests, constraints) with temporal buckets and simple load proxies (submits/schedules per minute, backlog estimate) and machine capacity hints.
- **Deterministic splits**: Based on hash of job_id to ensure stable, job-level separation of train/valid/test.
- **Robust training**: Hist-tree XGBoost with early stopping, clipping, and log1p to tame tails; evaluation back-transforms to seconds with clamping to keep metrics in the feasible range.

# Usage cheatsheet
- Full pipeline (reuse features if built):  
  `python -m scripts.run_pipeline --skip-validate --skip-features --features-path artifacts/features.parquet`
- Pre-train EDA only:  
  `python -m scripts.pretrain_analysis --features artifacts/features.parquet --clip-target 7200 --sample-cap 500000`
- Baselines:  
  `python -m scripts.baseline_metrics`
- Train only:  
  `python -m scripts.train_model --features artifacts/features.parquet --clip-target 7200`  
  Add `--no-log1p-target` to disable log transform.
- Evaluate only:  
  `python -m scripts.evaluate_model --features artifacts/features.parquet --model artifacts/xgb_queue_delay.json --clip-target 7200`
